<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Golang on skyduck</title>
    <link>https://skyduck-z.github.io/tags/golang/</link>
    <description>Recent content in Golang on skyduck</description>
    <image>
      <title>skyduck</title>
      <url>https://skyduck-z.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://skyduck-z.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.8</generator>
    <language>en</language>
    <lastBuildDate>Sat, 05 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://skyduck-z.github.io/tags/golang/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>go tips(持续更新)</title>
      <link>https://skyduck-z.github.io/posts/2025-06-22-go-tips/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://skyduck-z.github.io/posts/2025-06-22-go-tips/</guid>
      <description>&lt;p&gt;有很多从其他大佬的总结中抄来的，不注明出处了&lt;/p&gt;
&lt;p&gt;会记录一些:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有意义的小操作(可能有些比较tricky)&lt;/li&gt;
&lt;li&gt;容易理解偏差的点&lt;/li&gt;
&lt;li&gt;日常开发低频使用的知识点&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;golang代码执行顺序&#34;&gt;golang代码执行顺序&lt;/h2&gt;
&lt;p&gt;没有并发，一个顺序逻辑，CPU真正执行指令不一定与编码顺序完全一致，Go的编译器会做优化，前提是会解决依赖逻辑，看起来是“顺序执行”这一假设。&lt;/p&gt;
&lt;p&gt;了解更多可查看Golang内存模型规范。&lt;/p&gt;
&lt;h2 id=&#34;golang内存对齐&#34;&gt;golang内存对齐&lt;/h2&gt;
&lt;p&gt;内存对齐：数据在内存中的存储位置必须是特定字节数的倍数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU访问内存时，通常按固定大小的块（如4字节或8字节）读取，不对齐的数据可能导致需要两次访问&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;golang内存对齐：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础类型：通常是其自身大小（如int32为4字节，int64为8字节）&lt;/li&gt;
&lt;li&gt;结构体
&lt;ul&gt;
&lt;li&gt;每个字段按各自的对齐系数作对齐&lt;/li&gt;
&lt;li&gt;结构体自身的对齐系数为所有字段中最大的对齐系数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数组：与元素类型的对齐系数相同&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type demo1 struct {
	a int8
	b int16
	c int32
}

type demo2 struct {
	a int8
	c int32
	b int16
}

type demo3 struct {
	a int8
	c int32
	b int16
	d int16
}

func main() {
    fmt.Println(unsafe.Sizeof(demo1{})) // 8
	fmt.Println(unsafe.Sizeof(demo2{})) // 12
	fmt.Println(unsafe.Sizeof(demo3{})) // 12
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;demo1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a 是第一个字段，默认是已经对齐的，从第 0 个位置开始占据 1 字节&lt;/li&gt;
&lt;li&gt;b 是第二个字段，对齐倍数为 2，因此，必须空出 1 个字节，偏移量才是 2 的倍数，从第 2 个位置开始占据 2 字节&lt;/li&gt;
&lt;li&gt;c 是第三个字段，对齐倍数为 4，此时，内存已经是对齐的，从第 4 个位置开始占据 4 字节即可&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;demo2:&lt;/p&gt;</description>
    </item>
    <item>
      <title>golang singleflight</title>
      <link>https://skyduck-z.github.io/posts/2025-04-05-golang-singleflight/</link>
      <pubDate>Sat, 05 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://skyduck-z.github.io/posts/2025-04-05-golang-singleflight/</guid>
      <description>&lt;p&gt;高并发场景下，有时我们很多并发goroutine内有逻辑重合，可能拉了数据库同一张表的数据，可能拉了同一个远程图片，这些数据通常是偏静态的，那么没有必要在每个goroutine内部都做这个操作，可能会拉高上游服务的负载，可能会影响本地的执行效率，可能会浪费本地的网卡资源等。&lt;/p&gt;
&lt;p&gt;通常我们会依赖缓存来解决这个问题，这也确实是行之有效的；但是golang并发场景也给了我们更简便的实现方法：&lt;code&gt;golang.org/x/sync/singleflight&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;如下面的例子，&lt;code&gt;doSth&lt;/code&gt;有2s的Sleep模拟执行过程；我们分了两组for+routine，中间有1s的Sleep，确保第一个发起&lt;code&gt;doSth&lt;/code&gt;的&lt;code&gt;index&lt;/code&gt;是可控的，然后我们看到输出实际&lt;code&gt;doSth&lt;/code&gt;只执行了一次，所有routine共享了同一份结果。
PS：所有的&lt;code&gt;shared&lt;/code&gt;返回值都是&lt;code&gt;true&lt;/code&gt;，可能并不是大脑第一印象的第一个实际执行者是&lt;code&gt;false&lt;/code&gt;，其他人是&lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package main

import (
	&amp;#34;fmt&amp;#34;
	&amp;#34;sync&amp;#34;
	&amp;#34;sync/atomic&amp;#34;
	&amp;#34;time&amp;#34;
	&amp;#34;golang.org/x/sync/singleflight&amp;#34;
)

func doSth(index int) (any, error) {
	time.Sleep(2 * time.Second)
	return fmt.Sprintf(&amp;#34;result_%d&amp;#34;, index), nil
}

func main() {
	var g singleflight.Group
	var wg sync.WaitGroup
	var doCount int32 

	for i := 100; i &amp;lt; 101; i++ {
		wg.Add(1)
		index := i
		go func() {
			defer wg.Done()
			ret, err, shared := g.Do(&amp;#34;key&amp;#34;, func() (interface{}, error) {
				atomic.AddInt32(&amp;amp;doCount, 1)
				v,e := doSth(index)
				fmt.Printf(&amp;#34;index:%d, v: %v, err: %v\n&amp;#34;, index, v, e)
				return v,e
			})
			fmt.Printf(&amp;#34;index:%d, ret:%v, shared:%v, err:%v\n&amp;#34;, index, ret, shared, err)
		}()
	}
	
	
	time.Sleep(time.Second)
	
	for i := 0; i &amp;lt; 5; i++ {
		wg.Add(1)
		index := i
		go func() {
			defer wg.Done()
			ret, err, shared := g.Do(&amp;#34;key&amp;#34;, func() (interface{}, error) {
				atomic.AddInt32(&amp;amp;doCount, 1)
				v,e := doSth(index)
				fmt.Printf(&amp;#34;index:%d, v: %v, err: %v\n&amp;#34;, index, v, e)
				return v,e
			})
			fmt.Printf(&amp;#34;index:%d, ret:%v, shared:%v, err:%v\n&amp;#34;, index, ret, shared, err)
		}()
	}
	
	wg.Wait()
	
	fmt.Printf(&amp;#34;实际执行次数: %d\n&amp;#34;, doCount)
}

------
output:

index:100, v: result_100, err: &amp;lt;nil&amp;gt;
index:100, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
index:0, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
index:4, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
index:2, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
index:3, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
index:1, ret:result_100, shared:true, err:&amp;lt;nil&amp;gt;
实际执行次数: 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;真正发起共享的函数&lt;code&gt;Do&lt;/code&gt;入参是有&lt;code&gt;key&lt;/code&gt;的，我们把&lt;code&gt;key&lt;/code&gt;做个替换，会发现不同&lt;code&gt;key&lt;/code&gt;之间不共享，如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>golang select case机会均等</title>
      <link>https://skyduck-z.github.io/posts/2022-04-13-golang-select-case%E6%9C%BA%E4%BC%9A%E5%9D%87%E7%AD%89/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://skyduck-z.github.io/posts/2022-04-13-golang-select-case%E6%9C%BA%E4%BC%9A%E5%9D%87%E7%AD%89/</guid>
      <description>&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;
&lt;p&gt;我们每天都在使用下面这样的用法，对于这种多条件分支判定，我们从来不会指定权重，按固有思维，多条件间应该是机会均等的，那么golang如何做的呢？&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;select {
case &amp;lt;-ch1:
    do sth
case &amp;lt;-ch2:
    do sth
case &amp;lt;-ch3:
    do sth
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们先来看看到底是不是机会均等的。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package main

import(&amp;#34;fmt&amp;#34;)

func main() {
	sum1, sum2, sum3 := 0, 0, 0
	for loop:=0; loop&amp;lt;8 ; loop++ {
		count1, count2, count3 := 0, 0, 0
		ch1, ch2, ch3 := make(chan int), make(chan int), make(chan int)
		go func(){for { ch1&amp;lt;-1 }}()
		go func(){for { ch2&amp;lt;-1 }}()
		go func(){for { ch3&amp;lt;-1 }}()
		for i := 0; i &amp;lt; 10000; i++ {
			select {
			case &amp;lt;-ch1:
				count1++
				sum1++
			case &amp;lt;-ch2:
				count2++
				sum2++
			case &amp;lt;-ch3:
				count3++
				sum3++
			}
		}
		fmt.Println(&amp;#34;loop &amp;#34;, loop+1, &amp;#34;: &amp;#34;, count1, count2, count3)
	}
	fmt.Println(&amp;#34;sum: &amp;#34;, sum1, sum2, sum3)
}
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;结果:

loop  1 :  1535 5446 3019
loop  2 :  1607 5004 3389
loop  3 :  1146 3549 5305
loop  4 :  4100 2601 3299
loop  5 :  5938 2260 1802
loop  6 :  923 1268 7809
loop  7 :  4958 2273 2769
loop  8 :  3162 3432 3406
sum:  23369 25833 30798
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;诶，好像不太对啊，尤其&lt;code&gt;loop 7&lt;/code&gt;，偏差到飞起了啊&amp;hellip;总量的偏差也大的离谱&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Golang schedule</title>
      <link>https://skyduck-z.github.io/posts/2021-08-21-go-schedule/</link>
      <pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://skyduck-z.github.io/posts/2021-08-21-go-schedule/</guid>
      <description>&lt;h2 id=&#34;写在前面&#34;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;runtime包实现了所有&lt;code&gt;goroutine scheduler&lt;/code&gt;、&lt;code&gt;memory allocator&lt;/code&gt;、&lt;code&gt;garbage collector&lt;/code&gt;细节，理论上可以从runtime包获取一切信息，没有直接怼源码，而是站在巨人的肩膀上（直接吃大佬们吃剩下的）。&lt;/p&gt;
&lt;p&gt;搜集到的材料，大家都是基于不同的go版本做的分析，而go版本迭代调度算法也在持续更新，所以整理的可能有些乱。但是可以保证的是，所有材料都是GM-&amp;gt;GMP演化后的材料。&lt;/p&gt;
&lt;h2 id=&#34;gm&#34;&gt;GM&lt;/h2&gt;
&lt;p&gt;go1.1版本以前，调度使用GM模型，如下图所示。简单的理解GM模型，就是有一个始终执行的调度函数schedule不停的执行调度计算，当某个M的G执行完成了，调度器就把这个M放回M队列，可绑定执行其他G（如果某个M+G发生了syscall，那么本来并发度是通过M数量控制的，此时并发度就降低了？）；如果G执行过程中创建新的G，会将新的G放入到G全局可执行队列中。G全局可执行队列的操作有一把全局锁，这导致了各个M对G全局队列的操作存在严重的竞争。&lt;/p&gt;
&lt;p&gt;下面这段完全是我的臆测，请别太相信：&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;简单概括呢，所以可以认为有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G全局可执行队列(以下也可能简称G可执行队列)&lt;/li&gt;
&lt;li&gt;M可用队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;调度器要做的事就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从G的可执行队列取G并从M的可用队列取M，将二者绑定开始执行G&lt;/li&gt;
&lt;li&gt;对于已经执行完的G，销毁G并立即将M释放回M可用队列供后续使用&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;GM-model&#34; loading=&#34;lazy&#34; src=&#34;https://skyduck-z.github.io/images/go-schedule/gm.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么GM模型有哪些问题呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;strong&gt;重点问题&lt;/strong&gt;)单一的全局mutex(sched.lock)和集中状态管理
&lt;ul&gt;
&lt;li&gt;mutex需要保护所有与全局goroutine队列相关操作(创建、完成、重排等等)，竞争严重&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(&lt;strong&gt;重点问题&lt;/strong&gt;)per-M内存(M.mcache)问题
&lt;ul&gt;
&lt;li&gt;每个M都需要一个mcache，会导致资源消耗过大(每个mcache可以吸纳到2MB的内存缓存和其他缓存)
&lt;ul&gt;
&lt;li&gt;举个栗子，一个陷入syscall的M并不需要使用cache，但是在全部的M中，陷入系统调用的M与执行goroutine的M的比例可能是&lt;code&gt;N:1(N&amp;gt;&amp;gt;1)&lt;/code&gt;，这就导致了&lt;code&gt;N/(N+1)&lt;/code&gt;比例的mcache在闲置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据局部性差&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;举个栗子，M1执行G1，此时创建了G2，G2通常是立刻进入了G全局可执行队列，而此时M1还在执行G1，所以G2通常被其他M执行，但是G1和G2通常强相关，所以G2最好也在M1上执行，因为G2对M1的缓存命中率更高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;goroutine传递问题
&lt;ul&gt;
&lt;li&gt;goroutine(G)交接(G.nextg)，M之间会经常交接可运行的G&lt;/li&gt;
&lt;li&gt;再通俗点说，就是G空转，本来能够好好在一个M上执行完，但是由于全局队列的存在，G一旦回全局队列了，下次就不知道被哪个M取走了，所以叫“空转”；M加载G的上下文是有开销的，所以空转会导致性能下降&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;频繁的线程阻塞/解阻塞
&lt;ul&gt;
&lt;li&gt;syscalls情况下，线程经常被阻塞和解阻塞，增加了很多额外开销&lt;/li&gt;
&lt;li&gt;通俗点说，M+G syscall，M阻塞，syscall完成后，M解阻塞继续执行G（如果是通过M数量控制并发度，这是不是就导致了并发度降低？）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gmp&#34;&gt;GMP&lt;/h2&gt;
&lt;p&gt;基于以上说的GM的问题，go1.1以后开始使用GMP调度模型。
G、M、P的定义如下(***/src/runtime/runtime2.go)。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic       *_panic // innermost panic - offset known to liblink
	_defer       *_defer // innermost defer
	m            *m      // current m; offset known to arm liblink
	sched        gobuf
	syscallsp    uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc    uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp     uintptr        // expected sp at top of stack, to check in traceback
	param        unsafe.Pointer // passed parameter on wakeup
	atomicstatus uint32
	stackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid         int64
	schedlink    guintptr
	waitsince    int64      // approx time when the g become blocked
	waitreason   waitReason // if status==Gwaiting

	preempt       bool // preemption signal, duplicates stackguard0 = stackpreempt
	preemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule
	preemptShrink bool // shrink stack at synchronous safe point

	// asyncSafePoint is set if g is stopped at an asynchronous
	// safe point. This means there are frames on the stack
	// without precise pointer information.
	asyncSafePoint bool

	paniconfault bool // panic (instead of crash) on unexpected fault address
	gcscandone   bool // g has scanned stack; protected by _Gscan bit in status
	throwsplit   bool // must not split stack
	// activeStackChans indicates that there are unlocked channels
	// pointing into this goroutine&amp;#39;s stack. If true, stack
	// copying needs to acquire channel locks to protect these
	// areas of the stack.
	activeStackChans bool
	// parkingOnChan indicates that the goroutine is about to
	// park on a chansend or chanrecv. Used to signal an unsafe point
	// for stack shrinking. It&amp;#39;s a boolean value, but is updated atomically.
	parkingOnChan uint8

	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// Per-G GC state

	// gcAssistBytes is this G&amp;#39;s GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}

type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64       // for debuggers, but offset not hard-coded
	gsignal       *g           // signal-handling g
	goSigStack    gsignalStack // Go-allocated signal handling stack
	sigmask       sigset       // storage for saved signal mask
	tls           [6]uintptr   // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != &amp;#34;&amp;#34;, keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	freelink      *m // on sched.freem

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	// preemptGen counts the number of completed preemption
	// signals. This is used to detect when a preemption is
	// requested, but fails. Accessed atomically.
	preemptGen uint32

	// Whether this is a pending preemption signal on this M.
	// Accessed atomically.
	signalPending uint32

	dlogPerM

	mOS

	// Up to 10 locks held by this m, maintained by the lock ranking code.
	locksHeldLen int
	locksHeld    [10]heldLockInfo
}

type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	pcache      pageCache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready&amp;#39;d by
	// the current G and should be run next instead of what&amp;#39;s in
	// runq if there&amp;#39;s time remaining in the running G&amp;#39;s time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready&amp;#39;d
	// goroutines to the end of the run queue.
	runnext guintptr

	// Available G&amp;#39;s (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	// Cache of mspan objects from the heap.
	mspancache struct {
		// We need an explicit length here because this field is used
		// in allocation codepaths where write barriers are not allowed,
		// and eliminating the write barrier/keeping it eliminated from
		// slice updates is tricky, moreso than just managing the length
		// ourselves.
		len int
		buf [128]*mspan
	}

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// The when field of the first entry on the timer heap.
	// This is updated using atomic functions.
	// This is 0 if the timer heap is empty.
	timer0When uint64

	// Per-P GC state
	gcAssistTime         int64    // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64    // Nanoseconds in fractional mark worker (atomic)
	gcBgMarkWorker       guintptr // (atomic)
	gcMarkWorkerMode     gcMarkWorkerMode

	// gcMarkWorkerStartTime is the nanotime() at which this mark
	// worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P&amp;#39;s GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P&amp;#39;s GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	// Lock for timers. We normally access the timers while running
	// on this P, but the scheduler can also do it from a different P.
	timersLock mutex

	// Actions to take at some time. This is used to implement the
	// standard library&amp;#39;s time package.
	// Must hold timersLock to access.
	timers []*timer

	// Number of timers in P&amp;#39;s heap.
	// Modified using atomic instructions.
	numTimers uint32

	// Number of timerModifiedEarlier timers on P&amp;#39;s heap.
	// This should only be modified while holding timersLock,
	// or while the timer status is in a transient state
	// such as timerModifying.
	adjustTimers uint32

	// Number of timerDeleted timers in P&amp;#39;s heap.
	// Modified using atomic instructions.
	deletedTimers uint32

	// Race context used while executing timer functions.
	timerRaceCtx uintptr

	// preempt is set to indicate that this P should be enter the
	// scheduler ASAP (regardless of what G is running on it).
	preempt bool

	pad cpu.CacheLinePad
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;gmp模型的一些概念&#34;&gt;GMP模型的一些概念&lt;/h3&gt;
&lt;p&gt;上面M中有两个g需要关注下，curg和g0。
curg就是M当前绑定的G。
g0是带有调度栈的goroutine，普通的G的栈是分配在堆上的可增长的栈，而g0的栈是M对应的线程的栈。所有调度相关的代码，会先切换到该goroutine的栈中执行。即，线程的栈也是用的g实现，而不是使用的OS。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
